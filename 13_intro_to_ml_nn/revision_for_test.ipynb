{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91618ab1",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Nothing major, only gyaan on type of ML Models\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ad50f6",
   "metadata": {},
   "source": [
    "# Chapter 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre processing steps \n",
    "1 - Data encoding: Hot label endcoding and target encoding\n",
    "2 - Splitting the data: use sklearn for this - skl.model_selection import train_test_split\n",
    "3 - Scaling: again direct function from sklearn: minmacsclaer from skl.preprocessing\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315a4c8",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9284e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss function fro linear regression model - MSE/RSS (Mean squared error/residual square sum)\n",
    "R square: Basically how much better than simple mean model - Coefiicient of determination = 1-RSS/average model\n",
    "model credentials: lm.coef (basically feature importance) and lm.score (R square)\n",
    "\n",
    "Gradient descent: Consider a univariate function: y=wx+w0\n",
    "how does gradient descent work: Imagine a quadratic curve for a loss function for each of the Ws. \n",
    "    Minimum of these respective loss functions for each Ws will be at slope(derivative)=0.\n",
    "    Now, for each W, we want to start at random value of W and move towards 0 slope (derivative)\n",
    "    Imagine, 1 single W, if slope is positive, value of w needs to be reduced and if slope  is negative, W needs to be increased (ref 1)\n",
    "    In fact, bigger the value of slope(derivative), farther it is from minima point.\n",
    "    to achieve ref 1 , we subtract value of slope*factor from the current w\n",
    "    As an example for previous line, lets assume, minimum slope comes at w=2 and we are standing at w=7, \n",
    "        slope will be very positive (lets say 2) and to move towards 2, we subtract 2*0.1 (0.2) from 7 which is equal to 6.8. \n",
    "        Now slope will be lesser (lets say) for w=6.8, (lets say 1.8) and hence we will subtract 0.18 from 6.8\n",
    "        Notice that steps are getting smaller and smaller as we move towards minima point\n",
    "\n",
    "loss function: sum((y-y_hat)*2)/n = sum((y-(wT.x+w0))*2)/n = sum((y-(w1x1+w2x2+w3x3+----+w0))*2)/n\n",
    "\n",
    "Now to move towards values of w for minimum y, we do partial differentiation for y wrt w\n",
    "\n",
    "dy/dw1 = sum(2*(y-(wTx+w0))*(-x1))/n\n",
    "dy/dw2 = sum(2*(y-(wTx+w0))*(-x2))/n\n",
    "----\n",
    "dy/dw0= sum(2*(y-(wTx+w0))*(-1))/n\n",
    "\n",
    "Now, for every iteration, we need to subtract nw' from w: w-nw'\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f65d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
