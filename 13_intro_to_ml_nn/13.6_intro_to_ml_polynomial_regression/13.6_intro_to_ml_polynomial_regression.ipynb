{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Batch vs mini batch vs stochastic\n",
    "    Epoch: 1 full iteration of changing Ws (weights) for a linear regresion\n",
    "\n",
    "    types of 1 epoch\n",
    "        Batch gradient - Complete processing of full data for each iteration i.e epoch\n",
    "        mini batch gradient - Division of full data into mini batches (n) and for each batch weights are adjusted. \n",
    "            one epoch is completed when all batches have been processed at least once to change weights n times\n",
    "        Stochastic gradient- for each record, weights are adjusted\n",
    "\n",
    "    For batch, gradient descent works directly towards minima point. For mini batch, it goes zig zag and for stochastic, it goes extremely zig zag\n",
    "\n",
    "Polynomial regression\n",
    "    create new feature with square of the ploynomial feature and then optimize weight for it. NOTE THAT BOTH PARAMETERS x and x^2 will stay as features\n",
    "    use from sklearn.preprocessing import PolynomialFeatures to add polynomial features. NOTE THAT THIS CREATES HIGHER DEGREE FEATURES FOR ALL THE FEATURES. \n",
    "        FOR ONLY 1 COLUMN, DO IT MANUALLY USING HSTACK \n",
    "\n",
    "Application of polynomial regression in next chapter\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
