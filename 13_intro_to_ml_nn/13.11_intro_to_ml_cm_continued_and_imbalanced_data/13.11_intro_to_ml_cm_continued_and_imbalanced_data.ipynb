{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "True positive rate = Recall = TP/(TP+FN)\n",
    "\n",
    "False positive rate: FP/(FP+TN)\n",
    "\n",
    "How to choose best threshold\n",
    "    This should be based on the use case: Are we prioritizing precision, recall or balance of both (F1). Track these metrics against various thresholds\n",
    "\n",
    "    Techniques for thresholding:\n",
    "    ROC - AUC: ROC Area under curve\n",
    "        This is curve of TPR and FPR for all possible thresholds. Threshold with maximum area under curve tells us the best threshold\n",
    "        However, this creates issues with only limited values: for example [0.9,0.95],[0.1,0.05] will have very extreme thresholds\n",
    "        This tends to overestimate the model because of the aggregated performance for all thresholds\n",
    "    \n",
    "    Precision - recall curve\n",
    "        This is the best way to do it\n",
    "    \n",
    "\n",
    "ROC AUC is also used to choose the best model: for example choosing between logistic vs random forest vs knn\n",
    "\n",
    "Techniques to handle imbalanced data \n",
    "    Undersampling - Removes random samples from majority class\n",
    "    Oversampling - Duplicates values for minority class\n",
    "    SMOTE - Creates fake caluclated values for minority class - BEST WAY TO DO IT\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
